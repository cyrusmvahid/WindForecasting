{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\npd.__version__","metadata":{"execution":{"iopub.status.busy":"2023-09-25T21:54:25.464065Z","iopub.execute_input":"2023-09-25T21:54:25.465689Z","iopub.status.idle":"2023-09-25T21:54:25.475679Z","shell.execute_reply.started":"2023-09-25T21:54:25.465629Z","shell.execute_reply":"2023-09-25T21:54:25.474200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gluonts[mxnet] ","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:08:41.475739Z","iopub.execute_input":"2023-09-26T00:08:41.476046Z","iopub.status.idle":"2023-09-26T00:08:55.644055Z","shell.execute_reply.started":"2023-09-26T00:08:41.476021Z","shell.execute_reply":"2023-09-26T00:08:55.642981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n%reset -f\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport datetime as dt\nimport matplotlib.dates as mdates\nfrom gluonts.dataset.field_names import FieldName\nfrom gluonts.dataset.common import ListDataset\nfrom gluonts.dataset.pandas import PandasDataset\nfrom gluonts.dataset.util import to_pandas\nfrom gluonts.dataset.split import split\n\nimport calendar\nimport matplotlib.cm as cm\nfrom gluonts.mx import DeepAREstimator, Trainer\nfrom gluonts.dataset.split import TrainingDataset, TestData, TestTemplate\nfrom pandas import Period","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:08:55.645618Z","iopub.execute_input":"2023-09-26T00:08:55.646672Z","iopub.status.idle":"2023-09-26T00:08:57.947611Z","shell.execute_reply.started":"2023-09-26T00:08:55.646642Z","shell.execute_reply":"2023-09-26T00:08:57.946859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIRECTORY = \"/kaggle/input/GEF2012-wind-forecasting/\"\nimport os\nfrom glob import glob\nCSV_FILES = glob(os.path.join(DATA_DIRECTORY, '*.csv'))\nCSV_FILES.sort()\nfor i in range(len(CSV_FILES)):\n    CSV_FILES[i] = CSV_FILES[i].rsplit('/', 1)[1]\nCSV_FILES\n\nMAJOR_SEPARATOR = \"=\" * 30\nMINOR_SEPARATOR = \"-\" * 20\n\nDATE_FORMAT = '%Y%m%d%H'\nMONTHS = calendar.month_name\nCSV_FILES","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:08:57.948936Z","iopub.execute_input":"2023-09-26T00:08:57.949611Z","iopub.status.idle":"2023-09-26T00:08:57.958178Z","shell.execute_reply.started":"2023-09-26T00:08:57.949586Z","shell.execute_reply":"2023-09-26T00:08:57.957234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(DATA_DIRECTORY+CSV_FILES[2], index_col='date', date_format=DATE_FORMAT)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:08:58.876221Z","iopub.execute_input":"2023-09-26T00:08:58.876531Z","iopub.status.idle":"2023-09-26T00:08:58.974315Z","shell.execute_reply.started":"2023-09-26T00:08:58.876508Z","shell.execute_reply":"2023-09-26T00:08:58.973219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:00.540041Z","iopub.execute_input":"2023-09-26T00:09:00.540373Z","iopub.status.idle":"2023-09-26T00:09:00.565254Z","shell.execute_reply.started":"2023-09-26T00:09:00.540349Z","shell.execute_reply":"2023-09-26T00:09:00.564097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gluonts.dataset.split import TrainingDataset, TestData, TestTemplate\nfrom pandas import Period\n\nclass PlotSplitData():\n    \n    def __init__(self):\n        self._plt = plt\n        self._plt.rcParams[\"axes.grid\"] = True\n        self._plt.rcParams[\"figure.figsize\"] = (20, 3) \n  \n    def _highlight_entry(self, entry, color):\n        start = entry[\"start\"]\n        end = entry[\"start\"] + len(entry[\"target\"])\n        self._plt.axvspan(start, end, facecolor=color, alpha=0.2)\n\n\n    def plot_dataset_splitting(self, original_dataset, training_dataset, test_pairs):\n        for original_entry, train_entry in zip(original_dataset, training_dataset):\n            to_pandas(original_entry).plot()\n            self._highlight_entry(train_entry, \"red\")\n            self._plt.legend([\"sub dataset\", \"training dataset\"], loc=\"upper left\")\n            self._plt.show()\n\n        for original_entry in original_dataset:\n            for test_input, test_label in test_pairs:\n                to_pandas(original_entry).plot()\n                self._highlight_entry(test_input, \"green\")\n                self._highlight_entry(test_label, \"blue\")\n                self._plt.legend([\"sub dataset\", \"test input\", \"test label\"], loc=\"upper left\")\n                self._plt.show()\n\nclass TSDataException(BaseException):\n    def __init__(self, m):\n        self.message = m\n    def __str__(self):\n        return self.message\n    \nclass TSData:\n    def __init__(self, df: pd.DataFrame, expected_freq='1H') -> bool:\n        self._df = df\n        self._freq = pd.infer_freq(self._df.index) if pd.infer_freq(self._df.index) is not None else expected_freq\n        all_idx =  pd.date_range(start=self._df.index[0], end=self._df.index[-1], freq=self._freq)\n        self._diagnostics = {\"sorted\" : self._df.index.is_monotonic_increasing | self._df.index.is_monotonic_decreasing, \n                            \"ascending\": self._df.index.is_monotonic_increasing, \n                            \"descending\": self._df.index.is_monotonic_decreasing, \n                            \"frequency\": self._freq, \n                            \"count\": self._df[self._df.columns[0]].count(), \n                            \"null_count\": self._df[self._df.columns[0]].isna().sum(), \n                            \"missing_ts_count\": len(all_idx.difference(self._df.index))}\n        self._dataset = None\n        self._ds_split_parameters = None\n        \n    @property\n    def dataframe(self) -> pd.DataFrame:\n        return self._df\n    \n    #inferred frequency of the dataset\n    @property\n    def frequency(self):\n        return self._freq\n    \n    #Checking if the data is sorted and ascending and is not missing any timesteps\n    @property \n    def is_data_ts_ready(self) -> bool:\n        retval = True\n        retval = retval & self._df.index.is_monotonic_increasing\n        retval = retval & (pd.infer_freq(self._df.index) is not None)\n        retval = retval & self._df.index.is_unique\n        return retval\n    \n    #index of all the missing timesteps\n    @property\n    def missing_timesteps(self) -> pd.DatetimeIndex:\n        all_idx =  pd.date_range(start=self._df.index[0], end=self._df.index[-1], freq=self._freq)\n        return all_idx.difference(self._df.index)\n    \n    #dataframe of all the missing timesteps\n    @property\n    def missing_data(self) -> pd.DataFrame:\n        all = self.add_missing_timesteps()\n        return all.loc[all.index.intersection(self.missing_timesteps)]\n        \n    # returning information about the dataframe that can be used to troubleshoot behaviour of the data\n    @property\n    def diagnostics(self) -> dict:\n        return self._diagnostics\n\n    #inserting timesteps with nan values for all the missing timesteps to the dataframe. This does not alter the class, just returns a news dataframe\n    def add_missing_timesteps(self) -> pd.DataFrame :\n        all_idx =  pd.date_range(start=self._df.index[0], end=self._df.index[-1], freq=self._freq)\n        return self._df.reindex(all_idx)    \n    \n                \n    #gluonts's PandasDataset that can be used with utilities to split the data. It can also be used for training with various models that are provided for fine-tuning\n    @property\n    def dataset(self) -> PandasDataset:\n        if not self.is_data_ts_ready:\n            raise TSDataException(\"Unfortunately the dataset is not ready. Check diagnostics:]n{}\".format(self.diagnostics))\n        else:\n            self._dataset = PandasDataset(dict(self.dataframe))\n            return self._dataset\n        \n    #parameters based on which the dataset is split between test adn training\n    @property\n    def ds_split_parameters(self) -> dict:\n        if self._ds_split_parameters is None:\n            raise TSDataException(\"dataset parameters are not yet set. They will be set only after dataset is split to training and test using split_data()\")\n        return self._ds_split_parameters\n    \n    def aggregate_to_period(self, period='M') -> pd.DataFrame:\n        return self.dataframe.groupby(by=self.dataframe.index.to_period(period))\n        \n    def save_dataframe(self, location:str, kind:str=\"csv\") -> bool:\n        kind = kind.lower()\n        match kind:\n            case \"csv\":\n                self.dataframe.to_csv(location)\n            case \"pkl\":\n                print(kind)\n            case _:\n                raise TSDataException(\"unsupported file type: {}\".format(kind))\n        return True\n    \n    \n    #splits the dataset and returns test and training. Please note percentage is how much of the data will be used for training. The rest goes to test.\n    #There are three types of split: static, that only splits the data based on the date for the start of training. overlapping and distance based create test data based on overlapiing test pairs. I have not yet implemented \n    #the latter two methods\n    def split_data(self, prediction_length:int, method:str ='static', percentage:int=80, windows:int=3, distance:int=0, plot_data=False) -> (PandasDataset, PandasDataset):\n        if (percentage < 1) | (percentage > 100):\n            raise TSDataException(\"percentage of test data split must be between 1 and 100; {} is an invalid input\".format(percentage))\n        if windows <= 0:\n            raise TSDataException(\"you must provide a window > 0\")\n        split_point = int(len(self.dataframe.index) * percentage/100)\n        split_point = list(self.dataframe.index)[split_point]\n        date = \"{}-{}-01 00:00\".format(split_point.year, split_point.month)  \n        period = Period(value=date, freq=self.frequency)\n        match method:\n            case 'static':\n                training_dataset, test_template = split(self.dataset, date=period)\n                test_pairs = test_template.generate_instances(prediction_length=prediction_length, windows=windows)\n                parameters = {\"method\": method, \"prediction_length\": prediction_length, \"windows\": windows}\n            case 'overlapping':\n                print(\"overlapping\")\n            case 'distance':\n                if distance <= 0:\n                    raise TSDataException(\"you must provide a distance > 0\")\n                print('distance')\n            case _:\n                    raise TSDataException(\"unsupported method: {}\".format(method))\n        if plot_data:\n            plotter = PlotSplitData()\n            plotter.plot_dataset_splitting(self.dataset, training_dataset, test_pairs)\n            \n        self._ds_split_parameters = parameters\n\n                    \n        print(\"your dataset was split baesd on these parameters: {}. If you would like to access the parameters use property TSData.ds_split_parameters\".format(parameters))\n        return (training_dataset.dataset, test_pairs.dataset)\n            \n                   \n    # plotting density ahd scatter plot of the data. It returns the dataset that can be used for further investigation.\n    def visualize_sampled_dataframe(self, frac:float=0.1, xtick_period:str='M') -> pd.DataFrame:\n        sampled_df = self.dataframe.sample(frac=frac)\n        if xtick_period != \"ignore\":\n            xticks = sampled_df.index.to_period(xtick_period).unique()\n        plot_cols = sampled_df.columns\n        sampled_df.plot(kind='kde', figsize=(15, 2))\n        fig, axes = plt.subplots(nrows=len(plot_cols), ncols=1, figsize=(15,15), sharex=True)\n\n        for i in range(len(plot_cols)):\n            if xtick_period != \"ignore\":\n                sampled_df.plot(y=plot_cols[i], use_index=True, ax=axes[i], xticks=xticks)\n            else:\n                sampled_df.plot(y=cols[i], use_index=True, ax=axes[i])\n        plt.show()\n        return sampled_df    ","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:02.079313Z","iopub.execute_input":"2023-09-26T00:09:02.079652Z","iopub.status.idle":"2023-09-26T00:09:02.108185Z","shell.execute_reply.started":"2023-09-26T00:09:02.079626Z","shell.execute_reply":"2023-09-26T00:09:02.106881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = TSData(train_df.loc[train_df.index.year<2011])\ntrain_data.diagnostics","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:06.019142Z","iopub.execute_input":"2023-09-26T00:09:06.019473Z","iopub.status.idle":"2023-09-26T00:09:06.032905Z","shell.execute_reply.started":"2023-09-26T00:09:06.019448Z","shell.execute_reply":"2023-09-26T00:09:06.031927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.save_dataframe('/kaggle/working/train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:07.509010Z","iopub.execute_input":"2023-09-26T00:09:07.509349Z","iopub.status.idle":"2023-09-26T00:09:07.584611Z","shell.execute_reply.started":"2023-09-26T00:09:07.509324Z","shell.execute_reply":"2023-09-26T00:09:07.583565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%reset_selective -f \"temp_&\"\ntemp_df = train_data.visualize_sampled_dataframe(frac=1.0)\nprint(temp_df.describe().T)\nprint(temp_df.info())\n%reset_selective -f \"temp_&\"","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:13.091412Z","iopub.execute_input":"2023-09-26T00:09:13.091758Z","iopub.status.idle":"2023-09-26T00:09:18.133201Z","shell.execute_reply.started":"2023-09-26T00:09:13.091730Z","shell.execute_reply":"2023-09-26T00:09:18.132011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dataframe.boxplot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:18.135495Z","iopub.execute_input":"2023-09-26T00:09:18.135941Z","iopub.status.idle":"2023-09-26T00:09:18.451790Z","shell.execute_reply.started":"2023-09-26T00:09:18.135903Z","shell.execute_reply":"2023-09-26T00:09:18.450853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TSData(train_data.aggregate_to_period('M').mean()).visualize_sampled_dataframe(frac=1)\n(train_data.aggregate_to_period('M').mean()).plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:18.452777Z","iopub.execute_input":"2023-09-26T00:09:18.453672Z","iopub.status.idle":"2023-09-26T00:09:18.792386Z","shell.execute_reply.started":"2023-09-26T00:09:18.453645Z","shell.execute_reply":"2023-09-26T00:09:18.791571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TSData(train_data.aggregate_to_period('M').mean()).visualize_sampled_dataframe(frac=1)\n(train_data.aggregate_to_period('D').mean()).plot()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:18.794483Z","iopub.execute_input":"2023-09-26T00:09:18.795650Z","iopub.status.idle":"2023-09-26T00:09:19.205890Z","shell.execute_reply.started":"2023-09-26T00:09:18.795617Z","shell.execute_reply":"2023-09-26T00:09:19.205149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, test_ds = train_data.split_data(prediction_length=3*24, percentage=80)\ntrain_ds, test_ds","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:19.207414Z","iopub.execute_input":"2023-09-26T00:09:19.208370Z","iopub.status.idle":"2023-09-26T00:09:19.229999Z","shell.execute_reply.started":"2023-09-26T00:09:19.208343Z","shell.execute_reply":"2023-09-26T00:09:19.229049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# given that test data is randomized and the dataset has 7 different series, we are going to end up with 22 plots. I have used only one of the time series to demonstrate how training pair sampling is conducted\n%reset_selective -f \"^temp_\"\ntemp_df = train_data.dataframe[['wp1']]\ntemp_data = TSData(temp_df)\ntemp_train_ds, temp_test_ds = temp_data.split_data(prediction_length=3*24, percentage=80, plot_data=True)\n%reset_selective -f \"^temp_\"","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:19.231136Z","iopub.execute_input":"2023-09-26T00:09:19.231456Z","iopub.status.idle":"2023-09-26T00:09:21.077881Z","shell.execute_reply.started":"2023-09-26T00:09:19.231431Z","shell.execute_reply":"2023-09-26T00:09:21.076754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gluonts.mx import DeepAREstimator, Trainer\nfrom gluonts.evaluation import make_evaluation_predictions, Evaluator\n\n\n\ndef train_and_predict(dataset, estimator):\n    predictor = estimator.train(dataset)\n    forecast_it, ts_it = make_evaluation_predictions(\n        dataset=dataset, predictor=predictor\n    )\n    evaluator = Evaluator(quantiles=(np.arange(20) / 20.0)[1:])\n    agg_metrics, item_metrics = evaluator(ts_it, forecast_it, num_series=len(dataset))\n    return agg_metrics[\"MSE\"]\n\n\nestimator = DeepAREstimator(\n    freq=train_ds.freq, prediction_length=train_data.ds_split_parameters[\"prediction_length\"], trainer=Trainer(epochs=1)\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:21.403554Z","iopub.execute_input":"2023-09-26T00:09:21.403886Z","iopub.status.idle":"2023-09-26T00:09:21.415010Z","shell.execute_reply.started":"2023-09-26T00:09:21.403860Z","shell.execute_reply":"2023-09-26T00:09:21.413813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = DeepAREstimator(freq=train_ds.freq, prediction_length=train_data.ds_split_parameters[\"prediction_length\"], trainer=Trainer(epochs=10))\npredictor = estimator.train(train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:09:23.045287Z","iopub.execute_input":"2023-09-26T00:09:23.045675Z","iopub.status.idle":"2023-09-26T00:14:11.782460Z","shell.execute_reply.started":"2023-09-26T00:09:23.045644Z","shell.execute_reply":"2023-09-26T00:14:11.781756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"forecast_it, ts_it = make_evaluation_predictions(dataset=test_ds, predictor=predictor)\nforecasts = list(forecast_it)\ntests = list(ts_it)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:14:11.783912Z","iopub.execute_input":"2023-09-26T00:14:11.784453Z","iopub.status.idle":"2023-09-26T00:14:12.743652Z","shell.execute_reply.started":"2023-09-26T00:14:11.784430Z","shell.execute_reply":"2023-09-26T00:14:12.742876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N=7\nn_plot = 3\nindices = np.random.choice(np.arange(0, N), size=n_plot, replace=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:14:12.751461Z","iopub.execute_input":"2023-09-26T00:14:12.752013Z","iopub.status.idle":"2023-09-26T00:14:12.757349Z","shell.execute_reply.started":"2023-09-26T00:14:12.751979Z","shell.execute_reply":"2023-09-26T00:14:12.755699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(n_plot, 1, figsize=(20, n_plot * 3))\n\n\nfor index, ax in zip(indices, axes):\n    ax.plot(tests[index][-4 * train_data.ds_split_parameters[\"prediction_length\"] :].to_timestamp())\n    plt.sca(ax)\n    forecasts[index].plot(intervals=(0.9,), color=\"g\")\n    plt.legend([\"observed\", \"predicted median\", \"90% prediction interval\"])","metadata":{"execution":{"iopub.status.busy":"2023-09-26T00:14:12.763601Z","iopub.execute_input":"2023-09-26T00:14:12.763896Z","iopub.status.idle":"2023-09-26T00:14:13.789366Z","shell.execute_reply.started":"2023-09-26T00:14:12.763873Z","shell.execute_reply":"2023-09-26T00:14:13.788368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}